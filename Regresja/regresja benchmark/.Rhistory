objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
eta_vector = c(0.025, 0.05, 0.1, 0.2)
maxdepth_vector = c(1, 2, 3, 5, 10)
n_rounds = 120
best_params = c(0, 0, 0) # nrounds, eta, maxdepth
data_matrix = matrix(unlist(dane_raw.train.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
test_data_matrix = matrix(unlist(dane_raw.test.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
current_min = 1000000
# szukanie optymalnych parametrów
for (eta_val in eta_vector){
for (maxdepth_val in maxdepth_vector){
set.seed(12345)
temp_cv = xgb.cv(data = xgb.DMatrix(data = data_matrix, label = dane_raw.train.y),
nrounds = n_rounds,
max_depth = maxdepth_val,
eta = eta_val,
nfold = 10,
lambda = 1.5,
objective = "reg:linear",
verbose = FALSE)
temp_min = min(temp_cv$evaluation_log$test_rmse_mean)
if(temp_min < current_min){
current_min = temp_min
best_params[1] = which.min(temp_cv$evaluation_log$test_rmse_mean)
best_params[2] = eta_val
best_params[3] = maxdepth_val
}
}
}
# budowanie modelu
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
eta_vector = c(0.025, 0.05, 0.1, 0.2)
maxdepth_vector = c(6, 10, 15, 20)#c(1, 2, 3, 5, 10)
n_rounds = 120
best_params = c(0, 0, 0) # nrounds, eta, maxdepth
data_matrix = matrix(unlist(dane_raw.train.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
test_data_matrix = matrix(unlist(dane_raw.test.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
current_min = 1000000
# szukanie optymalnych parametrów
for (eta_val in eta_vector){
for (maxdepth_val in maxdepth_vector){
set.seed(12345)
temp_cv = xgb.cv(data = xgb.DMatrix(data = data_matrix, label = dane_raw.train.y),
nrounds = n_rounds,
max_depth = maxdepth_val,
eta = eta_val,
nfold = 10,
lambda = 1.5,
objective = "reg:linear",
verbose = FALSE)
temp_min = min(temp_cv$evaluation_log$test_rmse_mean)
if(temp_min < current_min){
current_min = temp_min
best_params[1] = which.min(temp_cv$evaluation_log$test_rmse_mean)
best_params[2] = eta_val
best_params[3] = maxdepth_val
}
}
}
# budowanie modelu
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
best_params
eta_vector = c(0.025, 0.05, 0.1, 0.2)
maxdepth_vector = c(2, 3, 5, 6)
n_rounds = 120
best_params = c(0, 0, 0) # nrounds, eta, maxdepth
data_matrix = matrix(unlist(dane_raw.train.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
test_data_matrix = matrix(unlist(dane_raw.test.bez.y), ncol = ncol(dane_raw.train.bez.y), byrow = FALSE)
current_min = 1000000
# szukanie optymalnych parametrów
for (eta_val in eta_vector){
for (maxdepth_val in maxdepth_vector){
set.seed(12345)
temp_cv = xgb.cv(data = xgb.DMatrix(data = data_matrix, label = dane_raw.train.y),
nrounds = n_rounds,
max_depth = maxdepth_val,
eta = eta_val,
nfold = 10,
lambda = 1.5,
objective = "reg:linear",
verbose = FALSE)
temp_min = min(temp_cv$evaluation_log$test_rmse_mean)
if(temp_min < current_min){
current_min = temp_min
best_params[1] = which.min(temp_cv$evaluation_log$test_rmse_mean)
best_params[2] = eta_val
best_params[3] = maxdepth_val
}
}
}
# budowanie modelu
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
best_params
# budowanie modelu
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = 150,# best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = 200,# best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = 100,# best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
model.xgboost = xgb.train(max_depth = best_params[3],
eta = best_params[2],
nrounds = 80,# best_params[1],
objective = "reg:linear",
data = xgb.DMatrix(data = data_matrix,
label = dane_raw.train.y)#,
#lambda = 1.5
)
# predykcja
pred.xgboost = predict(model.xgboost, test_data_matrix)
print("xgboost")
MSE(dane_raw.test.y, pred.xgboost)
pred.p(dane_raw.test.y, pred.xgboost)
library(mlbench)
dataset = as.data.frame(mlbench.friedman1(1000, sd=1))
View(dataset)
library(mlbench)
# wczytanie danych
dataset = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
library(mlbench)
# wczytanie danych
dane = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
library(mlbench)
# wczytanie danych
dane = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
tuning = tune.svm(y~.,
data = dane_train,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
tuning = tune.svm(y~.,
data = dane.train,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
1:10
dane[,1 3]
dane[,1;3]
dane[,1]
dane[,c(1,3)]
dane[, -c(2,4,5,6,7,8,9,10)]
dane[,c(1, 2, 3, 4) - c(1, 3)]
dane[,c(1, 2, 3, 4) - c(1, 3)]
dane[,c(1, 2, 3) - 3]
temp_varset = 1:ncol(data)
temp_varset = c(1:ncol(data))
temp_varset = 1:ncol(dane)
typeof(temp_varset)
temp_varset[temp_varset != 5]
tuning$performances$error
tuning$performances$error[1]
testlist = list()
testlist[1] = 0
testlist
testlist[3] = 5
testlist
testlist = v[0]
testlist <- v[0]
testlist <- c()
testlist
testlist[1] = 0
testlist
testlist[3] = 0
testlist
# funkcja do eliminacji zmiennych
backward_elimination_svm = function(data, target){
var_names = names(data)
variables_ranking = c()
classification_errors = c()
# find the best model
tuning = tune.svm(y~.,
data = dane.train,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
best_cost = tuning$best.parameters$cost
best_gamma = tuning$best.parameters$gamma
varset = 1:ncol(data)
for(i in 1:(ncol(data)-1)){
min_err = 2
for(var_num in varset){
temp_varset = varset[varset != var_num]
temp_tuning = tune.svm(y~.,
data = dane.train[,temp_varset],
cost = best_cost,
gamma = best_gamma,
kernel = "radial")
if(temp_tuning$performances$error[1] < min_err){
variables_ranking[ncol(dane)+1-i] = var_names[var_num]
classification_errors[ncol(dane)+1-i] = temp_tuning$performances$error[1]
next_varset = temp_varset
}
}
varset = next_varset
}
}
backward_elimination_svm = function(data, target){
var_names = names(data)
variables_ranking = c()
classification_errors = c()
# find the best model
tuning = tune.svm(y~.,
data = data,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
best_cost = tuning$best.parameters$cost
best_gamma = tuning$best.parameters$gamma
varset = 1:ncol(data)
for(i in 1:(ncol(data)-1)){
min_err = 2
for(var_num in varset){
temp_varset = varset[varset != var_num]
temp_tuning = tune.svm(y~.,
data = data[,temp_varset],
cost = best_cost,
gamma = best_gamma,
kernel = "radial")
if(temp_tuning$performances$error[1] < min_err){
variables_ranking[ncol(data)+1-i] = var_names[var_num]
classification_errors[ncol(data)+1-i] = temp_tuning$performances$error[1]
next_varset = temp_varset
}
}
varset = next_varset
}
}
backward_elimination_svm(dane.train.bez.y, dane.train.y)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# wczytanie danych
dane = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# funkcja do eliminacji zmiennych
backward_elimination_svm = function(data, target){
var_names = names(data)
variables_ranking = c()
classification_errors = c()
input_dataset = as.data.frame(cbind(data, target))
# find the best model
tuning = tune.svm(y~.,
data = input_dataset,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
best_cost = tuning$best.parameters$cost
best_gamma = tuning$best.parameters$gamma
varset = 1:ncol(data)
for(i in 1:(ncol(data)-1)){
min_err = 2
for(var_num in varset){
temp_varset = varset[varset != var_num]
temp_tuning = tune.svm(y~.,
data = data[,temp_varset],
cost = best_cost,
gamma = best_gamma,
kernel = "radial")
if(temp_tuning$performances$error[1] < min_err){
variables_ranking[ncol(data)+1-i] = var_names[var_num]
classification_errors[ncol(data)+1-i] = temp_tuning$performances$error[1]
next_varset = temp_varset
}
}
varset = next_varset
}
}
backward_elimination_svm(dane.train.bez.y, dane.train.y)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# wczytanie danych
dane = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# funkcja do eliminacji zmiennych
backward_elimination_svm = function(data, target){
var_names = names(data)
variables_ranking = c()
classification_errors = c()
input_dataset = as.data.frame(cbind(data, target))
names(input_dataset)[ncol(input_dataset)] = 'y'
# find the best model
tuning = tune.svm(y~.,
data = input_dataset,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
best_cost = tuning$best.parameters$cost
best_gamma = tuning$best.parameters$gamma
varset = 1:ncol(data)
for(i in 1:(ncol(data)-1)){
min_err = 2
for(var_num in varset){
temp_varset = varset[varset != var_num]
temp_tuning = tune.svm(y~.,
data = data[,temp_varset],
cost = best_cost,
gamma = best_gamma,
kernel = "radial")
if(temp_tuning$performances$error[1] < min_err){
variables_ranking[ncol(data)+1-i] = var_names[var_num]
classification_errors[ncol(data)+1-i] = temp_tuning$performances$error[1]
next_varset = temp_varset
}
}
varset = next_varset
}
}
backward_elimination_svm(dane.train.bez.y, dane.train.y)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# wczytanie danych
dane = as.data.frame(mlbench.friedman1(1000, sd=1))
#podział na zbiory: uczący i testowy
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# funkcja do eliminacji zmiennych
backward_elimination_svm = function(data, target){
var_names = names(data)
variables_ranking = c()
classification_errors = c()
input_dataset = as.data.frame(cbind(data, target))
names(input_dataset)[ncol(input_dataset)] = 'y'
# find the best model
tuning = tune.svm(y~.,
data = input_dataset,
cost = c(0.01, 0.1, 1, 10, 100, 1000),
gamma = c(0.01, 0.1, 0.1, 1, 3, 5, 7),
kernel = "radial")
best_cost = tuning$best.parameters$cost
best_gamma = tuning$best.parameters$gamma
varset = 1:ncol(data)
for(i in 1:(ncol(data)-1)){
min_err = 2
for(var_num in varset){
temp_varset = varset[varset != var_num]
input_dataset = as.data.frame(cbind(data[temp_varset], target))
names(input_dataset)[ncol(input_dataset)] = 'y'
temp_tuning = tune.svm(y~.,
data = input_dataset,
cost = best_cost,
gamma = best_gamma,
kernel = "radial")
if(temp_tuning$performances$error[1] < min_err){
variables_ranking[ncol(data)+1-i] = var_names[var_num]
classification_errors[ncol(data)+1-i] = temp_tuning$performances$error[1]
next_varset = temp_varset
}
}
varset = next_varset
}
}
backward_elimination_svm(dane.train.bez.y, dane.train.y)
