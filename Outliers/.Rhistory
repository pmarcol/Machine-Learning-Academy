type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
ranking
View(ranking)
sum(ranking[,2]>12)
which(ranking[,2]>12)
ranking[1,which(ranking[,2]>12)]
ranking[which(ranking[,2]>12),1]
View(dane.train)
dane.train.bez_outlierow = dane.train[-ranking[which(ranking[,2]>12),1],]
sum(ranking[,2]>12)
ranking[which(ranking[,2]>12),1]
View(ranking)
dane.train[ranking[which(ranking[,2]>12),1],]
View(dane.train)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
rownames(dane.train) = 1:nrow(dane.train)
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
ranking[which(ranking[,2]>12),1]
View(dane.train)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
sum(ranking[,2]>12)
View(ranking)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
rownames(dane.train) = 1:nrow(dane.train)
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane.train))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
sum(ranking[,2]>12)
View(ranking)
sum(ranking[,2]>8)
ranking[which(ranking[,2]>12),1]
ranking[which(ranking[,2]>12),1]
ranking[which(ranking[,2]>8),1]
dane.train.bez_outlierow = dane.train[-ranking[which(ranking[,2]>8),1],]
593-546
View(dane.train.bez_outlierow)
View(dane)
tuning = tune.randomForest(y~.,
data = dane.train.bez_outlierow,
ntree = c(100, 150, 200),
mtry = c(1, 2))
summary(tuning)
tuned_model = tuning$best.model
pred = predict(tuned_model, dane.test.bez.y, type='class')
con(pred, dane.test.y)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# funkcja do tablicy kontyngencji:
con=function(x,y){
tab=table(x,y)
print(tab)
cat("error rate = ", round(100*(1-sum(x==y)/length(x)),2),"%\n")
invisible()
}
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane.train))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
sum(ranking[,2]>8)
ranking[which(ranking[,2]>8),1]
dane.train.bez_outlierow = dane.train[-ranking[which(ranking[,2]>8),1],]
tuning = tune.randomForest(y~.,
data = dane.train.bez_outlierow,
ntree = c(100, 150, 200),
mtry = c(1, 2))
summary(tuning)
tuned_model = tuning$best.model
pred = predict(tuned_model, dane.test.bez.y, type='class')
con(pred, dane.test.y)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# funkcja do tablicy kontyngencji:
MSE=function(response, predicted){ 	#mean square error (blad sredniokwadratowy)
MSE=sum((predicted-response)^2)/length(predicted)
print(paste("Model ma wartosc MSE =",MSE), quote=F)
return(MSE)
invisible()
}
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane.train))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
sum(ranking[,2]>8)
ranking[which(ranking[,2]>8),1]
dane.train.bez_outlierow = dane.train[-ranking[which(ranking[,2]>8),1],]
set.seed(12345)
RF.tuning = tune.randomForest(y~.,
data = dane.train.bez_outlierow,
ntree = c(100, 150, 200, 250),
mtry = c(1, 2, 5, 10)
)
summary(RF.tuning)
# budowanie modelu
model.RF = RF.tuning$best.model
# predykcja
pred.RF = predict(model.RF, dane.test.bez.y)
print("Random Forest - dane surowe")
MSE(dane.test.y, pred.RF)
pred.p(dane.test.y, pred.RF)
View(ranking)
ranking[which(ranking[,2]>8),1]
sum(ranking[,2]>8)
sum(ranking[,2]>9)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# funkcja do tablicy kontyngencji:
MSE=function(response, predicted){ 	#mean square error (blad sredniokwadratowy)
MSE=sum((predicted-response)^2)/length(predicted)
print(paste("Model ma wartosc MSE =",MSE), quote=F)
return(MSE)
invisible()
}
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
# usuwamy zmienna "pietro" - duzo braków danych
dane = dane[, -4]
# usuwamy braki danych
dane=dane[!apply(is.na(dane[,]),1,any),]
names(dane)[ncol(dane)] = 'y'
dane.bez.y = dane[, -ncol(dane)]
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
# usuwanie wartości odstających
gamma_par = c(0.01, 0.5, 1, 5)
nu_par = c(0.001, 0.005, 0.01, 0.05)
final = rep(0, nrow(dane.train))
for(gamma_val in gamma_par){
for(nu_val in nu_par){
model = svm( ~ .,
data = dane.train,
type = "one-classification",
gamma = gamma_val,
nu = nu_val,
kernel = "radial")
typical = predict(model, dane.train)
outSVC = which(typical==FALSE)
for(i in outSVC){
final[i] = final[i] + 1
}
}
}
ranking = cbind(order(-final), final[order(-final)])
sum(ranking[,2]>9)
indices = ranking[which(ranking[,2]>9),1]
dane.train.bez_outlierow = dane.train[-indices,]
set.seed(12345)
RF.tuning = tune.randomForest(y~.,
data = dane.train.bez_outlierow,
ntree = c(100, 150, 200, 250),
mtry = c(1, 2, 5, 10)
)
summary(RF.tuning)
# budowanie modelu
model.RF = RF.tuning$best.model
# predykcja
pred.RF = predict(model.RF, dane.test.bez.y)
print("Random Forest - dane surowe")
MSE(dane.test.y, pred.RF)
pred.p(dane.test.y, pred.RF)
library(mlbench)
library(stats)
library(nnet)
library(neuralnet)
library(e1071)
library(rpart)
library(randomForest)
library(gbm)
library(FNN)
library(xgboost)
# MSE
MSE=function(response, predicted){ 	#mean square error (blad sredniokwadratowy)
MSE=sum((predicted-response)^2)/length(predicted)
print(paste("Model ma wartosc MSE =",MSE), quote=F)
return(MSE)
invisible()
}
##### wczytanie danych #####
dane = read.csv2(file = "Ceny_transakcyjne_1m2mieszkan_ze_zm_sztucznymi.csv", sep = ";", row.names = 1)
##### dzielenie na podzbiory #####
set.seed(1234)
train=sample(1:nrow(dane), 0.67*nrow(dane))
dane.train = dane[train, ]
dane.test = dane[-train, ]
dane.train.bez.y = dane.train[, -ncol(dane.train)]
dane.train.y = dane.train[, ncol(dane.train)]
dane.test.bez.y = dane.test[, -ncol(dane.test)]
dane.test.y = dane.test[, ncol(dane.test)]
sum(is.na(dane$Pietro))
for(i in 1:ncol(dane)){}
for(i in 1:ncol(dane)){
print(colnames(dane)[i])
print(sum(is.na(dane[,i])))
}
braki = rep(0, ncol(dane.train))
for(i in 1:ncol(dane.train)){
braki[i] = sum(is.na(dane.train[,i]))
}
braki
order(braki)
order(-braki)
which(braki>0)
braki = rep(0, ncol(dane.train))
for(i in 1:ncol(dane.train)){
braki[i] = sum(is.na(dane.train[,i]))
}
braki = order(-braki)
ile = length(which(braki>0))
do_zmiany = braki[1:ile]
do_zmiany
braki = rep(0, ncol(dane.train))
for(i in 1:ncol(dane.train)){
braki[i] = sum(is.na(dane.train[,i]))
}
ile = length(which(braki>0))
do_zmiany = braki[1:ile]
do_zmiany
braki = rep(0, ncol(dane.train))
for(i in 1:ncol(dane.train)){
braki[i] = sum(is.na(dane.train[,i]))
}
ile = length(which(braki>0))
braki=order(-braki)
do_zmiany = braki[1:ile]
do_zmiany
